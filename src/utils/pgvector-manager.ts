#!/usr/bin/env tsx

import "dotenv/config";

/**
 * PostgreSQL pgvectorãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ç®¡ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
 *
 * ä½¿ç”¨æ–¹æ³•:
 * - npm run pgvector:update - src/data/ä»¥ä¸‹ã®å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’æ›´æ–°
 * - npm run pgvector:stats - ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
 * - npm run pgvector:rebuild - ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’å®Œå…¨ã«å†æ§‹ç¯‰
 */

import fs from "node:fs/promises";
import path from "node:path";
import { resolveFromProjectRoot } from "./path-utils.js";
import { getPostgresVector } from "../mastra/config/vectors.js";
import { getEmbeddingModel } from "../mastra/config/models.js";

const INDEX_NAME = "knowledge_base";

// ä¸­å¤®è¨­å®šã®PostgreSQL pgvectorè¨­å®šã‚’ä½¿ç”¨
const vectorStore = getPostgresVector();

// ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
const embeddingModel = getEmbeddingModel();

/**
 * ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã«å¤‰æ›
 */
async function generateEmbeddings(texts: string[]) {
  const embeddings = [];

  console.log(`ğŸ”„ Generating embeddings for ${texts.length} chunks...`);

  for (let i = 0; i < texts.length; i++) {
    const text = texts[i];
    try {
      const result = await embeddingModel.doEmbed({
        values: [text],
      });
      embeddings.push(result.embeddings[0]);

      if ((i + 1) % 10 === 0) {
        console.log(
          `  - Progress: ${i + 1}/${texts.length} embeddings generated`
        );
      }
    } catch (error) {
      console.error(`Error generating embedding for chunk ${i}:`, error);
      throw error;
    }
  }

  return embeddings;
}

/**
 * ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
 */
function splitTextIntoChunks(
  text: string,
  chunkSize = 200,
  overlap = 50
): string[] {
  const chunks: string[] = [];
  let start = 0;

  while (start < text.length) {
    const end = Math.min(start + chunkSize, text.length);
    const chunk = text.slice(start, end);
    chunks.push(chunk.trim());

    if (end >= text.length) break;
    start = end - overlap;
  }

  return chunks;
}

/**
 * ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å­˜åœ¨ç¢ºèª
 */
async function checkIfIndexExists(): Promise<boolean> {
  try {
    await vectorStore.query({
      indexName: INDEX_NAME,
      queryVector: new Array(1536).fill(0),
      topK: 1,
    });
    return true;
  } catch {
    return false;
  }
}

/**
 * ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¦‹ã¤ã‘ã‚‹
 */
async function findDataDirectory(): Promise<string> {
  const possiblePaths = [
    await resolveFromProjectRoot("src/data").catch(() => null),
    path.resolve("src/data"),
    path.resolve("../../src/data"),
    "src/data",
  ].filter(Boolean) as string[];

  for (const tryPath of possiblePaths) {
    try {
      await fs.access(tryPath);
      const stats = await fs.stat(tryPath);
      if (stats.isDirectory()) {
        return tryPath;
      }
    } catch {
      // continue
    }
  }

  throw new Error("Could not find src/data directory in any expected location");
}

/**
 * ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
 */
async function loadAllDataFiles(): Promise<
  Array<{ content: string; filename: string; path: string }>
> {
  const dataDir = await findDataDirectory();
  console.log(`ğŸ“ Scanning data directory: ${dataDir}`);

  const files = await fs.readdir(dataDir);
  const textFiles = files.filter(
    (file) =>
      file.endsWith(".txt") ||
      file.endsWith(".md") ||
      file.endsWith(".json") ||
      file.endsWith(".csv")
  );

  if (textFiles.length === 0) {
    throw new Error(`No text files found in ${dataDir}`);
  }

  console.log(`ğŸ“„ Found ${textFiles.length} files: ${textFiles.join(", ")}`);

  const fileData = [];
  for (const filename of textFiles) {
    const filePath = path.join(dataDir, filename);
    try {
      const content = await fs.readFile(filePath, "utf-8");
      fileData.push({ content, filename, path: filePath });
      console.log(`  âœ… Loaded: ${filename} (${content.length} characters)`);
    } catch (error) {
      console.warn(`  âš ï¸  Failed to load ${filename}:`, error);
    }
  }

  return fileData;
}

/**
 * ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’æ›´æ–°
 */
async function updateVectorStore() {
  try {
    console.log("ğŸ”„ Updating PostgreSQL pgvector store...");

    // ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    const fileData = await loadAllDataFiles();

    // å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒ£ãƒ³ã‚¯ã‚’åé›†
    const allChunks: Array<{
      text: string;
      source: string;
      fileIndex: number;
      chunkIndex: number;
    }> = [];

    for (let fileIndex = 0; fileIndex < fileData.length; fileIndex++) {
      const { content, filename } = fileData[fileIndex];
      const chunks = splitTextIntoChunks(content);

      for (let chunkIndex = 0; chunkIndex < chunks.length; chunkIndex++) {
        allChunks.push({
          text: chunks[chunkIndex],
          source: filename,
          fileIndex,
          chunkIndex,
        });
      }

      console.log(`  ğŸ“Š ${filename}: ${chunks.length} chunks`);
    }

    console.log(`ğŸ“Š Total chunks from all files: ${allChunks.length}`);

    // ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
    const indexExists = await checkIfIndexExists();

    if (!indexExists) {
      console.log("ğŸ”§ Creating PostgreSQL pgvector index...");
      await vectorStore.createIndex({
        indexName: INDEX_NAME,
        dimension: 1536,
      });
      console.log("âœ… PostgreSQL pgvector index created");
    } else {
      console.log("â„¹ï¸  Using existing PostgreSQL pgvector index");
    }

    // ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ
    const texts = allChunks.map((chunk) => chunk.text);
    const embeddings = await generateEmbeddings(texts);

    // ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æŒ¿å…¥/æ›´æ–°
    console.log("ğŸ’¾ Inserting documents into PostgreSQL...");
    const ids = allChunks.map((_, index) => `doc_${index}`);
    const metadata = allChunks.map((chunk, index) => ({
      text: chunk.text,
      source: chunk.source,
      file_index: chunk.fileIndex,
      chunk_index: chunk.chunkIndex,
      global_index: index,
      updated_at: new Date().toISOString(),
    }));

    await vectorStore.upsert({
      indexName: INDEX_NAME,
      vectors: embeddings,
      metadata: metadata,
      ids: ids,
    });

    console.log(
      `âœ… Successfully updated PostgreSQL pgvector store with ${allChunks.length} documents from ${fileData.length} files`
    );

    // ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã®çµ±è¨ˆã‚’è¡¨ç¤º
    const fileStats = fileData.map((file, index) => {
      const fileChunks = allChunks.filter((chunk) => chunk.fileIndex === index);
      return `  ğŸ“„ ${file.filename}: ${fileChunks.length} chunks`;
    });
    console.log("ğŸ“ˆ File breakdown:");
    for (const stat of fileStats) {
      console.log(stat);
    }
  } catch (error) {
    console.error("âŒ Error updating vector store:", error);
    process.exit(1);
  }
}

/**
 * ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
 */
async function showStats() {
  try {
    console.log("ğŸ“Š PostgreSQL pgvector Store Statistics");
    console.log("=========================================");

    const connectionInfo = {
      connectionString:
        process.env.PG_CONNECTION_STRING ||
        "postgresql://postgres:password@localhost:5432/mastra_vectors",
      indexName: INDEX_NAME,
    };

    console.log(`Connection: ${connectionInfo.connectionString}`);
    console.log(`Index Name: ${connectionInfo.indexName}`);

    const indexExists = await checkIfIndexExists();
    console.log(`Index Exists: ${indexExists ? "âœ… Yes" : "âŒ No"}`);

    if (indexExists) {
      console.log("Embedding Dimension: 1536");
      console.log("Model: text-embedding-3-small");
      console.log("Vector Database: PostgreSQL with pgvector extension");
    } else {
      console.log(
        "â„¹ï¸  Run 'npm run pgvector:update' to create the vector store"
      );
    }
  } catch (error) {
    console.error("âŒ Error getting stats:", error);
    process.exit(1);
  }
}

/**
 * ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’å®Œå…¨ã«å†æ§‹ç¯‰
 */
async function rebuildVectorStore() {
  try {
    console.log("ğŸ”„ Rebuilding PostgreSQL pgvector store...");

    // æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å‰Šé™¤
    try {
      await vectorStore.deleteIndex({
        indexName: INDEX_NAME,
      });
      console.log("ğŸ—‘ï¸  Existing index deleted");
    } catch {
      console.log("â„¹ï¸  No existing index to delete");
    }

    // æ–°ã—ããƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ
    await updateVectorStore();
  } catch (error) {
    console.error("âŒ Error rebuilding vector store:", error);
    process.exit(1);
  }
}

// ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’å‡¦ç†
const command = process.argv[2];

switch (command) {
  case "update":
    await updateVectorStore();
    break;
  case "stats":
    await showStats();
    break;
  case "rebuild":
    await rebuildVectorStore();
    break;
  default:
    console.log("PostgreSQL pgvector Store Manager");
    console.log("==================================");
    console.log("Available commands:");
    console.log(
      "  update  - Update vector store with all files from src/data/"
    );
    console.log("  stats   - Show vector store statistics");
    console.log("  rebuild - Completely rebuild vector store");
    console.log("");
    console.log("Usage:");
    console.log("  npm run pgvector:update");
    console.log("  npm run pgvector:stats");
    console.log("  npm run pgvector:rebuild");
    break;
}
